{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2, os, sys, torch\n",
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration, LongT5Config, BatchEncoding\n",
    "my_dir = sys.path[0]\n",
    "sys.path.append(os.path.join(my_dir, 'src/data'))\n",
    "sys.path.append(os.path.join(my_dir, 'src/models'))\n",
    "from make_dataset import *\n",
    "from train_model import *\n",
    "from torch.utils.data import  DataLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikipedia/20220301.en to C:/Users/nguye/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 15.3k/15.3k [00:00<00:00, 344kB/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solo ys khong\n"
     ]
    }
   ],
   "source": [
    "test = \"\"\"solo ys khong\"\"\"\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.create_documents([test])\n",
    "for i in texts:\n",
    "    print(i.page_content)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type longt5. This is not supported for all configurations of models and can yield errors.\n",
      "10it [00:29,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Why is Google Fibre taking so long to roll out? [SEP] that the company announced the expansion of its beta service to cover 3 cities: Boston, Los Angeles, and Washington, DC, the speeds were still limited to up to 200 Mbit/s. In June 2016, Google Fiber acquired Webpass to boost its effort in its experiments with wireless technologies. As a result, Google Fiber put its effort on fiber to the premises on hold to explore more on the cheaper wireless alternative. By early 2017, the Webpass division of Google Fiber expanded 1 Gbit/s wireless service to customers in many cities in the United States. In November 2016, Atlas Networks, an ISP that serves -26.567433416333188 [SEP]</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "(\"One does not simply lay down a large fiber network. First, you have to have the money. That's not really an issue for Google. Then, you have to convince municipal governments to let you build a network, and you have to get past the incumbent ISP, who wants to keep their monopoly intact. You have to find enough subscribers, you have to find people to build the network, you have to do customer service and installation, and you have to not be hated by the public. Throwing money at those problems is ineffective.\",)\n",
      "It's because it's a way to increase the speed of the service. It's a way to increase the speed of the service. It's a way to increase the speed of the service. It's a way to increase the speed of the service.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_path = 'docs/ELI5_val.jsonl'\n",
    "with open(valid_path, 'r') as json_file:\n",
    "        json_list_valid = list(json_file)\n",
    "valid_dataset = ELI5(json_list_valid, 'val')\n",
    "bs = 1\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=bs,\\\n",
    "        shuffle=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"t5-small\")\n",
    "config = LongT5Config.from_pretrained(\"t5-small\")\n",
    "# model = LongT5ForConditionalGeneration.from_pretrained(\n",
    "#     \"model.pt\", config=config)\n",
    "model = torch.load('model.pt')\n",
    "\n",
    "for i, batch in tqdm(enumerate(valid_data_loader)):\n",
    "    if i < 10:\n",
    "        continue    \n",
    "    input_token = batch[0].input_ids.squeeze(0).to('cuda')\n",
    "    input_mask = batch[0].attention_mask.squeeze(0).to('cuda')\n",
    "    ans_token = batch[1]\n",
    "#     print(tokenizer.decode(batch[0].input_ids, skip_special_tokens=True))\n",
    "    output = model.module.generate(\n",
    "            input_ids = input_token,\n",
    "            attention_mask = input_mask,\n",
    "            max_length=128,\n",
    "            early_stopping=True,\n",
    "            num_beams=5)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(tokenizer.decode(batch[0]['input_ids'].squeeze(0)[0]))\n",
    "    print(ans_token)\n",
    "    print(answer)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create database\n",
    "def create_database():\n",
    "    # TABLE 2 columns\n",
    "    # 1 text, 1 embedding \n",
    "    commands = (\n",
    "        \"\"\"\n",
    "        create table vendor (\n",
    "            vendor_id serial primary key,\n",
    "            vendor_name varchar(255) not null,\n",
    "        )\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        create table parts (\n",
    "            part_id serial primary key,\n",
    "            part_name varchar(255) not null,\n",
    "        )\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        create table part_drawings (\n",
    "            part_id \n",
    "        )\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_part(part_name):\n",
    "    insert_part = \"INSERT INTO parts(part_name) VALUES(%s) RETURNING part_id;\"\n",
    "conn = None\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host='localhost',\n",
    "        port=\"5432\",\n",
    "        dbname=\"suppliers\",\n",
    "        user=\"postgres\",\n",
    "        password=1)\n",
    "    cur = conn.cursor()\n",
    "    # cur.execute(\"\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "except psycopg2.DatabaseError as error:\n",
    "    print(error)\n",
    "finally:\n",
    "    # print(\"Done!\")\n",
    "    if conn is not None:\n",
    "        conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtech-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
